<head>

    <style>
        #left{
            position: absolute;
            width: 50vw;
            height: 100vh;
            left: 0px;
            color: red;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        #right{
            position: absolute;
            width: 50vw;
            height: 100vh;
            right: 0px;
            color: red;
            display: flex;
            justify-content: center;
            align-items: center;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script>
        let videoEl;
    
        const MODEL_WIDTH = 256;
        const MODEL_HEIGHT = 256;
        const canvas = document.createElement("canvas");
        canvas.width = MODEL_WIDTH;
        canvas.height = MODEL_HEIGHT;
        const ctx = canvas.getContext("2d");

        window.addEventListener("load", async () => {
            videoEl = document.getElementById("inputVideo");
            navigator.getUserMedia(
                { video: {} },
                stream => videoEl.srcObject = stream,
                err => console.error(err)
            )


            const canvasElement = document.querySelector(".output-canvas");
            const gl = canvasElement.getContext("webgl");
            await gl.makeXRCompatible();

            // Make sure to request a session with depth-sensing enabled
            const session = navigator.xr.requestSession("immersive-ar", {
                requiredFeatures: ["depth-sensing"],
                depthSensing: {
                    usagePreference: ["gpu-optimized"],
                    formatPreference: ["luminance-alpha"],
                },
            });

            const glBinding = new XRWebGLBinding(session, gl);

            session.requestAnimationFrame(rafCallback);
        });

        // Obtain depth information in an active and animated frame
        function rafCallback(time, frame) {
            session.requestAnimationFrame(rafCallback);
            const pose = frame.getViewerPose(referenceSpace);
            if (pose) {
                for (const view of pose.views) {
                    const depthInformation = glBinding.getDepthInformation(view);
                    if (depthInformation) {
                        console.log(depthInformation)
                    }
                }
            }
        }

        async function onPlay(params) {            
            // const session = await ort.InferenceSession.create(
            //     "https://huggingface.co/julienkay/sentis-MiDaS/resolve/main/onnx/midas_v21_small_256.onnx",
            //     { executionProviders: ["webgpu"] }
            // );

            // while(true){
            //     const inputTensor = getInputTensor(videoEl)
            //     const feeds = { input_image: inputTensor };
            //     const results = await session.run(feeds);
            //     const depth = results.output_depth.data;
            //     const centerDepth = getCenterDepth(depth, MODEL_WIDTH, MODEL_HEIGHT)
            //     const smoothDepth = smooth(centerDepth)
    
            //     console.log(smoothDepth);
            // }
        }

        
    
        function getCenterDepth(depth, width, height) {
            const x = Math.floor(width / 2);
            const y = Math.floor(height / 2);
            return depth[y * width + x];
        }
    
        let smoothedValue = 0;
        function smooth(value) {
            const alpha = 0.1;
            smoothedValue = smoothedValue * (1 - alpha) + value * alpha;
            return smoothedValue;
        }
    
        function getInputTensor(video) {
            // Draw video frame scaled to model size
            ctx.drawImage(video, 0, 0, MODEL_WIDTH, MODEL_HEIGHT);
    
            const imageData = ctx.getImageData(0, 0, MODEL_WIDTH, MODEL_HEIGHT);
            const { data } = imageData; // RGBA
    
            const floatData = new Float32Array(1 * 3 * MODEL_WIDTH * MODEL_HEIGHT);
    
            // Convert RGBA â†’ CHW format
            for (let i = 0; i < MODEL_WIDTH * MODEL_HEIGHT; i++) {
                const r = data[i * 4] / 255;
                const g = data[i * 4 + 1] / 255;
                const b = data[i * 4 + 2] / 255;
    
                floatData[i] = r;                                   // R channel
                floatData[i + MODEL_WIDTH * MODEL_HEIGHT] = g;       // G channel
                floatData[i + 2 * MODEL_WIDTH * MODEL_HEIGHT] = b;   // B channel
            }
    
            return new ort.Tensor("float32", floatData, [1, 3, MODEL_HEIGHT, MODEL_WIDTH]);
        }
    
        let left = 0;
        let right = 0;
    
        addEventListener("keydown", (event) => 
        { 
            if(event.key == "+"){
                left += 1;
                right += 1;
                document.getElementById("left").style.left = `${left}px`
                document.getElementById("right").style.right = `${right}px`
            }
            if(event.key == "-"){
                left -= 1;
                right -= 1;
                document.getElementById("left").style.left = `${left}px`
                document.getElementById("right").style.right = `${right}px`
            }
        });
    
    </script>
</head>

<body style="background: black; margin: 0; font-family: consolas; font-size: 100px;">
    <div id="left">
        <div>
            +
        </div>
    </div>
    <div id="right">
        <div>
            +
        </div>
    </div>
    <div>
        <video id="inputVideo" onplay="onPlay(this)" style="opacity: 1;" autoplay muted></video>
    </div>
</body>